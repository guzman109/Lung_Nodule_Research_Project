{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spiculation_Binary_Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ednX4-ck2iw8"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I9mh51swIao"
      },
      "source": [
        "# Spiculation Binary Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ednX4-ck2iw8"
      },
      "source": [
        "## Prologue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ9p1tkD2QqD"
      },
      "source": [
        "### Import Libraries and paths to files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZPan4XAviFi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import datetime\n",
        "\n",
        "# !pip -q install livelossplot \n",
        "# from livelossplot.tf_keras import PlotLossesCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6fjMehNvpXS"
      },
      "source": [
        "df_path = '/content/drive/MyDrive/Colab Notebooks/Research/parsedXMLnew.xlsx'\n",
        "ds_path = '/content/nodules'\n",
        "ds_file_1 = '/content/drive/MyDrive/Colab Notebooks/Research/dataset_before_bin.pickle'\n",
        "ds_file_2 = '/content/drive/MyDrive/Colab Notebooks/Research/dataset_3_class.pickle'\n",
        "ds_file_3 = '/content/drive/MyDrive/Colab Notebooks/Research/dataset_3_augmented.pickle'\n",
        "# ds_file_grad = '/content/drive/MyDrive/Colab Notebooks/Research/dataset_grad.pickle'\n",
        "save_path = '/content/drive/MyDrive/Colab Notebooks/Research/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3uBAx3bhPCw"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOVM8dcW5sPI"
      },
      "source": [
        "#### Global"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je-wUzCihoq5"
      },
      "source": [
        "def plot_heatmap(y_true, y_pred, class_names, ax, title):\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  sns.heatmap(\n",
        "      cm, \n",
        "      annot=True, \n",
        "      square=True, \n",
        "      xticklabels=class_names, \n",
        "      yticklabels=class_names,\n",
        "      fmt='d', \n",
        "      cmap=plt.cm.Blues,\n",
        "      cbar=False,\n",
        "      ax=ax\n",
        "  )\n",
        "  ax.set_title(title, fontsize=16)\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "  ax.set_ylabel('True Label', fontsize=12)\n",
        "  ax.set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "def disp_and_save_cam(img, gradcam, name, conv_layer_name):\n",
        "    save_path = '/content/drive/MyDrive/Colab Notebooks/Research/visuals/'\n",
        "\n",
        "    fig = plt.figure(figsize=(10,5))\n",
        "    fig.add_subplot(1,3,1, title='Image with CAM')\n",
        "    plt.imshow(img)\n",
        "    plt.imshow(gradcam, alpha=0.5)\n",
        "    plt.axis('off')\n",
        "\n",
        "    fig.add_subplot(1,3,2, title='Image')\n",
        "    plt.imshow(img)\n",
        "    plt.imshow(img, alpha=0.5)\n",
        "    plt.axis('off')\n",
        "\n",
        "    fig.add_subplot(1,3,3, title='CAM')\n",
        "    plt.imshow(gradcam)\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f'{save_path}{conv_layer_name}/{name}.png')\n",
        "    plt.show()\n",
        "def gradcam(img, model, last_conv_layer_name, label, show=False, name=''):\n",
        "  last_conv_layer = model.get_layer(last_conv_layer_name)\n",
        "  last_conv_layer_model = tf.keras.Model(model.inputs, last_conv_layer.output)\n",
        "\n",
        "  classifier_input = tf.keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "  x = classifier_input \n",
        "  begin = False                \n",
        "  for count, layer in enumerate(model.layers):\n",
        "    if last_conv_layer_name == layer.name:\n",
        "      begin = True\n",
        "    if begin:\n",
        "      x = layer(x)\n",
        "      \n",
        "  classifier_model = tf.keras.Model(classifier_input, x)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    inputs = img[np.newaxis, ...]\n",
        "    last_conv_layer_output = last_conv_layer_model(inputs)\n",
        "    tape.watch(last_conv_layer_output)\n",
        "    pred = classifier_model(last_conv_layer_output)\n",
        "    pred_class = 1 if tf.greater(pred, 0.5).numpy()[0,0] == True else 0\n",
        "\n",
        "  grads = tape.gradient(pred, last_conv_layer_output)\n",
        "  pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "\n",
        "  last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "  pooled_grads = pooled_grads.numpy()\n",
        "  for i in range(pooled_grads.shape[-1]):\n",
        "    last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "  \n",
        "  # Average over all the filters to get a single 2D array\n",
        "  gradcam = np.mean(last_conv_layer_output, axis=-1)\n",
        "  # Clip the values (equivalent to applying ReLU)\n",
        "  # and then normalise the values\n",
        "  gradcam = np.clip(gradcam, 0, np.max(gradcam)) / np.max(gradcam)\n",
        "  gradcam = cv2.resize(gradcam, img.shape[:2])\n",
        "\n",
        "  if show:\n",
        "    disp_and_save_cam(img, gradcam, name, last_conv_layer_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4C00mYk5c5U"
      },
      "source": [
        "#### Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QagUqT-Ry5FV"
      },
      "source": [
        "def create_model_1(input_shape, bin_act_func, optimizer, loss_func=''):\n",
        "  # VGG16 convolutional layers\n",
        "  conv_base = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "  # Freeze all conv_layers\n",
        "  for layer in conv_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Custom FC layers\n",
        "  top_model = tf.keras.Sequential(\n",
        "      [\n",
        "        tf.keras.layers.GlobalAveragePooling2D(name='GAP_layer'),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax', name='output_layer')\n",
        "      ]\n",
        "  )\n",
        "  # Concat both models into one.\n",
        "  model = tf.keras.Sequential()\n",
        "  for layer in conv_base.layers:\n",
        "    model.add(layer)\n",
        "  model.add(top_model)\n",
        "  model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def fine_tune_model_1(num_layers, input_shape, bin_act_func, optimizer, loss_func, fname):\n",
        "  # VGG16 convolutional layers\n",
        "  conv_base = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "  \n",
        "  num_frozen = len(conv_base.layers)-num_layers-1\n",
        "  # Freeze required conv_layers\n",
        "  for layer in conv_base.layers[:num_frozen]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Custom FC layers\n",
        "  top_model = tf.keras.Sequential(\n",
        "      [\n",
        "        tf.keras.layers.GlobalAveragePooling2D(name='GAP_layer'),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax', name='output_layer')\n",
        "      ]\n",
        "  )\n",
        "  \n",
        "  # Concat both models into one.\n",
        "  model = tf.keras.Sequential()\n",
        "  for layer in conv_base.layers:\n",
        "    model.add(layer)\n",
        "  model.add(top_model)\n",
        "  model.load_weights(fname)\n",
        "  model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBP03coI50Xn"
      },
      "source": [
        "#### Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF1LtYdo50Xn"
      },
      "source": [
        "def create_model_2(input_shape, dp_rates, bin_act_func, optimizer, loss_func=''):\n",
        "  # VGG16 convolutional layers\n",
        "  conv_base = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "  # Freeze all conv_layers\n",
        "  for layer in conv_base.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Custom FC layers\n",
        "  top_model = tf.keras.Sequential(\n",
        "      [\n",
        "        tf.keras.layers.GlobalAveragePooling2D(name='GAP_layer'),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dp_rates[0]),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dp_rates[1]),       \n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dp_rates[2]),\n",
        "        tf.keras.layers.Dense(3, activation='softmax', name='output_layer')\n",
        "      ]\n",
        "  )\n",
        "  # Concat both models into one.\n",
        "  model = tf.keras.Sequential()\n",
        "  for layer in conv_base.layers:\n",
        "    model.add(layer)\n",
        "  model.add(top_model)\n",
        "  model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def fine_tune_model_2(num_layers, input_shape, dp_rates, bin_act_func, optimizer, loss_func, fname):\n",
        "  # VGG16 convolutional layers\n",
        "  conv_base = tf.keras.applications.VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "  \n",
        "  num_frozen = len(conv_base.layers)-num_layers-1\n",
        "  # Freeze required conv_layers\n",
        "  for layer in conv_base.layers[:num_frozen]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  # Custom FC layers\n",
        "  top_model = tf.keras.Sequential(\n",
        "      [        \n",
        "        tf.keras.layers.GlobalAveragePooling2D(name='GAP_layer'),\n",
        "        tf.keras.layers.Dense(512, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dp_rates[0]),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dp_rates[1]),       \n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dp_rates[2]),\n",
        "        tf.keras.layers.Dense(3, activation='softmax', name='output_layer')\n",
        "      ]\n",
        "  )\n",
        "  \n",
        "  # Concat both models into one.\n",
        "  model = tf.keras.Sequential()\n",
        "  for layer in conv_base.layers:\n",
        "    model.add(layer)\n",
        "  model.add(top_model)\n",
        "  model.load_weights(fname)\n",
        "  model.compile(optimizer=optimizer, loss=loss_func, metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBU5EOplwJY_"
      },
      "source": [
        "## Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXyUL3AB05lK"
      },
      "source": [
        "### Extract Labels and Binning Them\n",
        "\n",
        "We only need the InstanceID (nodule id) and the Spiculation rating (label)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUe6e6SywWsV"
      },
      "source": [
        "# df = pd.read_excel(df_path)\n",
        "# df = df[['InstanceID', 'spiculation']]\n",
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RDaYqfHSh2F"
      },
      "source": [
        "Load the images and match their corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHYTCYzOPZfp"
      },
      "source": [
        "shapes = []\n",
        "ds = []\n",
        "if os.path.isfile(ds_file_1):\n",
        "  with open(ds_file_1, 'rb') as f:\n",
        "    ds = pickle.load(f)\n",
        "  for d in ds:\n",
        "    shapes.append(d[0].shape)\n",
        "  shapes = np.array(shapes)\n",
        "else:\n",
        "  !tar xzf '/content/drive/MyDrive/Colab Notebooks/Research/LIDC/nodules.tar.gz'\n",
        "\n",
        "  file_names = glob(os.path.join(ds_path,'*.png'))\n",
        "  id_set = set(df['InstanceID'])\n",
        "  for f in file_names:\n",
        "    id = int(f.split('/')[-1].split('.')[0])\n",
        "    # print('InstanceID:', id, end='\\n\\n')\n",
        "    if id in id_set:\n",
        "      label =  df[df['InstanceID']==id]['spiculation'].iloc[0]\n",
        "      img = cv2.imread(f, 0)\n",
        "      shapes.append(img.shape)\n",
        "      ds.append([img, label, id])\n",
        "  shapes = np.array(shapes)\n",
        "\n",
        "  with open(os.path.join(save_path,'dataset_before_bin.pickle'), 'wb') as f:\n",
        "    pickle.dump(ds, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-Wpck3P6sJB"
      },
      "source": [
        "Load dataset from pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFcKC8CfT6OR"
      },
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.title('Total Number of Images Before Binning')\n",
        "labels, counts = np.unique([d[1] for d in ds], return_counts='True')\n",
        "bars = plt.bar(labels, counts, align='center')\n",
        "plt.gca().set_xticks(labels)\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Spiculation Rating')\n",
        "for bar in bars:\n",
        "  yval = bar.get_height()\n",
        "  plt.text(bar.get_x()+0.3, yval + 200, yval)\n",
        "plt.savefig(os.path.join(save_path,'bar_before_bin'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB9Yl8VQwkTs"
      },
      "source": [
        "Spiculation rating is, 1 least spiculation, to, 5 most spiculation. As we can see 1 has the most frequency compared to 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGJRPxChrf1X"
      },
      "source": [
        "# df['spiculation'] = df['spiculation'].replace([2,4], [1,5])\n",
        "# df['spiculation'] = df['spiculation'].replace([1,3,5], [0,1,2])\n",
        "# df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjZCGa4bD25d"
      },
      "source": [
        "if os.path.isfile(ds_file_2):\n",
        "  with open(ds_file_2, 'rb') as f:\n",
        "    ds = pickle.load(f)\n",
        "else:\n",
        "  for d in ds:\n",
        "    d[1] =  df[df['InstanceID']==d[2]]['spiculation'].iloc[0]\n",
        "\n",
        "  with open(os.path.join(save_path,'dataset_3_class.pickle'), 'wb') as f:\n",
        "    pickle.dump(ds, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fDKaqn5HIHn"
      },
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.title('Total Number of Images After Bining')\n",
        "labels, counts = np.unique([d[1] for d in ds], return_counts='True')\n",
        "bars = plt.bar(labels, counts, align='center')\n",
        "plt.gca().set_xticks(labels)\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Spiculation Rating')\n",
        "for bar in bars:\n",
        "  yval = bar.get_height()\n",
        "  plt.text(bar.get_x()+0.3, yval + 200, yval)\n",
        "plt.savefig(os.path.join(save_path,'bar_bin'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPStKL1I-d_j"
      },
      "source": [
        "### Scaling Images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geth426IZAub"
      },
      "source": [
        "temp = []\n",
        "input_size = (128,128,3)\n",
        "min_size = (32,32)\n",
        "\n",
        "for d in ds:\n",
        "  if d[0].shape >= min_size:\n",
        "    if d[0].shape < input_size[:2]:\n",
        "      # Up-Scaling\n",
        "      d[0] = cv2.resize(d[0], dsize=input_size[:2], interpolation=cv2.INTER_CUBIC)\n",
        "    elif d[0].shape > input_size:\n",
        "      # Down-Scaling\n",
        "      d[0] = cv2.resize(d[0], dsize=input_size[:2], interpolation=cv2.INTER_AREA)\n",
        "    temp.append(d)\n",
        "\n",
        "del(ds)\n",
        "ds = temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz3is6ij9Idj"
      },
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.title(f'Total Number of Images After Bining and Images Greater than {min_size}')\n",
        "labels, counts = np.unique([d[1] for d in ds], return_counts='True')\n",
        "bars = plt.bar(labels, counts, align='center')\n",
        "plt.gca().set_xticks(labels)\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Spiculation Rating')\n",
        "for bar in bars:\n",
        "  yval = bar.get_height()\n",
        "  plt.text(bar.get_x()+0.3, yval + 10, yval)\n",
        "plt.savefig(os.path.join(save_path,'bar_bin'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM-cE2oBQABh"
      },
      "source": [
        "### Random Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOqZT-nIP7ls"
      },
      "source": [
        "class_0, class_1, class_2 = [], [], []\n",
        "for d in ds:\n",
        "  if d[1] == 0:\n",
        "    class_0.append(d.copy())\n",
        "  elif d[1] == 1:\n",
        "    class_1.append(d.copy())\n",
        "  else:\n",
        "    class_2.append(d.copy())\n",
        "del(ds)\n",
        "bin_0 = []\n",
        "bin_1 = class_1.copy()\n",
        "bin_2 = []\n",
        "\n",
        "# Indicies already sampled\n",
        "used = set()\n",
        "while len(bin_0) != len(bin_1):\n",
        "  idx = np.random.randint(low=0, high=len(class_0))\n",
        "  if idx not in used:\n",
        "    bin_0.append(class_0[idx].copy())\n",
        "    used.add(idx)\n",
        "\n",
        "used = set()\n",
        "while len(bin_2) != len(bin_1):\n",
        "  idx = np.random.randint(low=0, high=len(class_2))\n",
        "  if idx not in used:\n",
        "    bin_2.append(class_2[idx].copy())\n",
        "    used.add(idx)\n",
        "\n",
        "del(class_0)\n",
        "del(class_1)\n",
        "del(class_2)\n",
        "\n",
        "ds = bin_0.copy()\n",
        "ds.extend(bin_1.copy())\n",
        "ds.extend(bin_2.copy())\n",
        "\n",
        "del(bin_0)\n",
        "del(bin_1)\n",
        "del(bin_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UxRkJKBS8Dd"
      },
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.title(f'Total Number of Images After Bining and Images Greater than {min_size}')\n",
        "labels, counts = np.unique([d[1] for d in ds], return_counts='True')\n",
        "bars = plt.bar(labels, counts, align='center')\n",
        "plt.gca().set_xticks(labels)\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Spiculation Rating')\n",
        "for bar in bars:\n",
        "  yval = bar.get_height()\n",
        "  plt.text(bar.get_x()+0.3, yval + 10, yval)\n",
        "plt.savefig(os.path.join(save_path,'bar_bin'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w6alNC7SVLS"
      },
      "source": [
        "### Grayscale -> RGB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3tCFieoCnSo"
      },
      "source": [
        "for d in ds:\n",
        "  d[0] = np.dstack([d[0]]*3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adJ-5nlcVb0y"
      },
      "source": [
        "### Split Training and Test Sets\n",
        "Now that we have the dataset ready. Its time to split it into training and test sets, as well as randomizing and converting to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIuERrfTin1q"
      },
      "source": [
        "X, y = [], []\n",
        "for d in ds:\n",
        "  X.append(d[0])\n",
        "  y.append(d[1])\n",
        "\n",
        "X, y = np.array(X), tf.keras.utils.to_categorical(y, num_classes=3)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzROBPit062u"
      },
      "source": [
        "fig = plt.figure(figsize=(10,5))\n",
        "plt.title(f'Total Number of Images After Bining and Images Greater than {min_size}')\n",
        "labels, counts = np.unique(y_train, return_counts='True')\n",
        "bars = plt.bar(labels, counts, align='center')\n",
        "plt.gca().set_xticks(labels)\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Spiculation Rating')\n",
        "for bar in bars:\n",
        "  yval = bar.get_height()\n",
        "  plt.text(bar.get_x()+0.3, yval + 10, yval)\n",
        "plt.savefig(os.path.join(save_path,'bar_bin'))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_XewaK0djkF"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEcgVQk-Jiri"
      },
      "source": [
        "ImgGen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=360, shear_range=10, horizontal_flip=True, brightness_range=[0.5,1.5], vertical_flip=True, width_shift_range=.1, height_shift_range=.1, validation_split=.2, dtype=tf.uint8)\n",
        "train_generator = ImgGen.flow(X_train, y_train, batch_size=64, subset='training', seed=1)\n",
        "val_generator = ImgGen.flow(X_train, y_train, batch_size=64, subset='validation', seed=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXwgtme772Md"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJQGPX9jRnCz"
      },
      "source": [
        "learning_rate = 0.0001\n",
        "# batch_size = 64\n",
        "n_epochs = 100\n",
        "bin_act_func = 'softmax'\n",
        "loss_func = 'categorical_crossentropy'\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1aHvgC72V1g"
      },
      "source": [
        "### No Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYVz2HAW-CFM"
      },
      "source": [
        "# fine_tune = 3\n",
        "with tf.device('/GPU:0'):\n",
        "  model_1 = create_model_1(input_size, bin_act_func, optim, loss_func)\n",
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cSApFAte-ih-"
      },
      "source": [
        "# CallBacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model_1_history = model_1.fit(train_generator, validation_data=val_generator, epochs=n_epochs, verbose=2, callbacks=[early_stop, tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rjmQUmw-9btC"
      },
      "source": [
        "fname = os.path.join(save_path,f'weights/model_1_weights_no_fine_tuning.h5')\n",
        "model_1.save_weights(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tor5-gfP-7SA"
      },
      "source": [
        "y_pred_1 = tf.argmax(model_1.predict(X_test), axis=1)\n",
        "\n",
        "class_names = ['0','1']\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "plot_heatmap(y_test, y_pred_1, class_names, ax, title=\"Confusion Matrix for Model A\")    \n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=1.25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WJE-ezO0_OM3"
      },
      "source": [
        "print(classification_report(y_test, y_pred_1, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvbH_YvdHOZ5"
      },
      "source": [
        "### Fine Tuning Last Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "607wNTF_7TRC"
      },
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  model_1_ft1 = fine_tune_model_1(1,input_size, bin_act_func, optim, loss_func, fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SwRRMQpOgih"
      },
      "source": [
        "# CallBacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model_1_ft1_history = model_1_ft1.fit(train_generator, validation_data=val_generator, epochs=n_epochs, verbose=2, callbacks=[early_stop, tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJvPQcTCQyhn"
      },
      "source": [
        "fname = os.path.join(save_path,f'weights/model_1_weights_fine_tuning_1.h5')\n",
        "model_1_ft1.save_weights(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4AEXeWtQOSB"
      },
      "source": [
        "y_pred_1 = tf.greater(model_1_ft1.predict(X_test), 0.5)\n",
        "\n",
        "class_names = ['0','1']\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "plot_heatmap(y_test, y_pred_1, class_names, ax, title=\"Confusion Matrix for Model A\")    \n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=1.25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvD4dGIIQSlU"
      },
      "source": [
        "print(classification_report(y_test, y_pred_1, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z27-dUiZEth"
      },
      "source": [
        "### Fine Tuning Last 2 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkDt9cJNZEti"
      },
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  model_1_ft2 = fine_tune_model_1(2,input_size, bin_act_func, optim, loss_func, fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQXrBmyrZEti"
      },
      "source": [
        "# CallBacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model_1_ft1_history = model_1_ft2.fit(train_generator, validation_data=val_generator, epochs=n_epochs, verbose=2, callbacks=[early_stop, tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cngyB4u9ZEtj"
      },
      "source": [
        "fname = os.path.join(save_path,f'weights/model_1_weights_fine_tuning_2.h5')\n",
        "model_1_ft2.save_weights(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfAbkE2mZEtk"
      },
      "source": [
        "y_pred_1 = tf.greater(model_1_ft2.predict(X_test), 0.5)\n",
        "\n",
        "class_names = ['0','1']\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "plot_heatmap(y_test, y_pred_1, class_names, ax, title=\"Confusion Matrix for Model A\")    \n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=1.25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzsjSQkTZEtl"
      },
      "source": [
        "print(classification_report(y_test, y_pred_1, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPV6J3aNaqZm"
      },
      "source": [
        "### Fine Tuning Last 3 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11SSbeETaqZq"
      },
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  model_1_ft3 = fine_tune_model_1(3,input_size, bin_act_func, optim, loss_func, fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhYBDGN-aqZr"
      },
      "source": [
        "# CallBacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model_1_ft3_history = model_1_ft3.fit(train_generator, validation_data=val_generator, epochs=n_epochs, verbose=2, callbacks=[early_stop, tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFq2lZvQaqZu"
      },
      "source": [
        "fname = os.path.join(save_path,f'weights/model_1_weights_fine_tuning_3.h5')\n",
        "model_1_ft3.save_weights(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJOZTLaoaqZu"
      },
      "source": [
        "y_pred_1 = tf.greater(model_1_ft3.predict(X_test), 0.5)\n",
        "\n",
        "class_names = ['0','1']\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "plot_heatmap(y_test, y_pred_1, class_names, ax, title=\"Confusion Matrix for Model A\")    \n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=1.25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChRbmRYqaqZv"
      },
      "source": [
        "print(classification_report(y_test, y_pred_1, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbkYJKrz6dvc"
      },
      "source": [
        "## Model 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfCPy_ca6hbV"
      },
      "source": [
        "learning_rate = 0.0001\n",
        "n_epochs = 100\n",
        "bin_act_func = 'sigmoid'\n",
        "loss_func = 'categorical_crossentropy'\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHpEVZjZaXKh"
      },
      "source": [
        "### No Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ4y7W_b6vjg"
      },
      "source": [
        "dp_rates = [.5,.5,.5]\n",
        "with tf.device('/GPU:0'):\n",
        "  model_2 = create_model_2(input_size, dp_rates, bin_act_func, optim, loss_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKRIzpMhD4IB"
      },
      "source": [
        "# CallBacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model_2_history = model_2.fit(train_generator, validation_data=val_generator, epochs=n_epochs, verbose=2, callbacks=[early_stop, tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VGWDdeIES_u"
      },
      "source": [
        "fname = os.path.join(save_path,f'weights/model_2_weights_no_fine_tuning.h5')\n",
        "model_2.save_weights(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24oMj1oWEbEz"
      },
      "source": [
        "y_pred_2 = tf.greater(model_2.predict(X_test), 0.5)\n",
        "\n",
        "class_names = ['0','1']\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "plot_heatmap(y_test, y_pred_2, class_names, ax, title=\"Confusion Matrix for Model A\")    \n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=1.25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EKLlkgbEmqD"
      },
      "source": [
        "print(classification_report(y_test, y_pred_2, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfpl64pOGC9W"
      },
      "source": [
        "### Fine Tuning Last Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr5rzM9kGC9b"
      },
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  model_2_ft1 = fine_tune_model_2(1,input_size, dp_rates, bin_act_func, optim, loss_func, fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJaTzVPFGC9c"
      },
      "source": [
        "# CallBacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model_2_ft1_history = model_2_ft1.fit(train_generator, validation_data=val_generator, epochs=n_epochs, verbose=2, callbacks=[early_stop, tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O4gto3iGC9d"
      },
      "source": [
        "fname = os.path.join(save_path,f'weights/model_2_weights_fine_tuning_1.h5')\n",
        "model_2_ft1.save_weights(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzNeQwTVGC9d"
      },
      "source": [
        "y_pred_2 = tf.greater(model_2_ft1.predict(X_test), 0.5)\n",
        "\n",
        "class_names = ['0','1']\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "plot_heatmap(y_test, y_pred_2, class_names, ax, title=\"Confusion Matrix for Model A\")    \n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=1.25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nSnBIhEGC9e"
      },
      "source": [
        "print(classification_report(y_test, y_pred_2, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrFDPhUIr7h3"
      },
      "source": [
        "### Fine Tuning Last 2 Layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFsA2fCCr7h9"
      },
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  model_2_ft2 = fine_tune_model_2(1,input_size, dp_rates, bin_act_func, optim, loss_func, fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nLYO9mHr7h-"
      },
      "source": [
        "# CallBacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model_2_ft2_history = model_2_ft2.fit(train_generator, validation_data=val_generator, epochs=n_epochs, verbose=2, callbacks=[early_stop, tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiyFZov0r7iA"
      },
      "source": [
        "fname = os.path.join(save_path,f'weights/model_2_weights_fine_tuning_2.h5')\n",
        "model_2_ft2.save_weights(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix9nly1cr7iA"
      },
      "source": [
        "y_pred_2 = tf.greater(model_2_ft2.predict(X_test), 0.5)\n",
        "\n",
        "class_names = ['0','1']\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "plot_heatmap(y_test, y_pred_2, class_names, ax, title=\"Confusion Matrix for Model A\")    \n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=1.25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4USeX-Fr7iB"
      },
      "source": [
        "print(classification_report(y_test, y_pred_2, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWoBxrVzr8Tv"
      },
      "source": [
        "### Fine Tuning Last 3 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pqjIjPTr8Tv"
      },
      "source": [
        "with tf.device('/GPU:0'):\n",
        "  model_2_ft3 = fine_tune_model_2(1,input_size, dp_rates, bin_act_func, optim, loss_func, fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hgl6XEBr8Tw"
      },
      "source": [
        "# CallBacks\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, mode='min')\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "model_2_ft3_history = model_2_ft3.fit(train_generator, validation_data=val_generator, epochs=n_epochs, verbose=2, callbacks=[early_stop, tensorboard_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6ha1J7Cr8Tx"
      },
      "source": [
        "fname = os.path.join(save_path,f'weights/model_2_weights_fine_tuning_3.h5')\n",
        "model_2_ft3.save_weights(fname)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe53eZfIr8Tx"
      },
      "source": [
        "y_pred_2 = tf.greater(model_2_ft3.predict(X_test), 0.5)\n",
        "\n",
        "class_names = ['0','1']\n",
        "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
        "\n",
        "plot_heatmap(y_test, y_pred_2, class_names, ax, title=\"Confusion Matrix for Model A\")    \n",
        "\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=1.25)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZtfCiOhr8Ty"
      },
      "source": [
        "print(classification_report(y_test, y_pred_2, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lUSCwbwVFCX"
      },
      "source": [
        "## TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9993itM6LbR4"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "# Clear any logs from previous runs\n",
        "# %rm -rf ./logs/\n",
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kInBpFUf5ub"
      },
      "source": [
        "## Grad-CAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q95Tc-Iqphhx"
      },
      "source": [
        "model = model_2_ft2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO_4Bire7wp8"
      },
      "source": [
        "last_conv_layer = 'block1_conv2'\n",
        "# match = []\n",
        "# indicies = []\n",
        "# for idx,(img,label) in enumerate(zip(X_test,y_test)):\n",
        "#   i = np.random.randint(0,10)\n",
        "#   j = np.random.randint(0,10)\n",
        "#   show = False\n",
        "#   if i == j:\n",
        "#     match.append((img, label))\n",
        "#     indicies.append(idx)\n",
        "#     show = True\n",
        "#   gradcam(img, model, last_conv_layer, label, show=show, name=idx)\n",
        "\n",
        "#   # guided_gradcam(img, model, last_conv_layer, np.argmax(label), show=show)\n",
        "#   if len(match) == 10:\n",
        "#     break\n",
        "for idx, (img,label) in zip(indicies,match):\n",
        "  gradcam(img, model, last_conv_layer, np.argmax(label), show=True, name=idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTcLAbX7vN1"
      },
      "source": [
        "last_conv_layer = 'block2_conv2'\n",
        "for idx, (img,label) in zip(indicies,match):\n",
        "  gradcam(img, model, last_conv_layer, np.argmax(label), show=True, name=idx)\n",
        "  # guided_gradcam(img, model, last_conv_layer, np.argmax(label), show=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fn7-rH_7r4I"
      },
      "source": [
        "last_conv_layer = 'block3_conv3'\n",
        "for idx, (img,label) in zip(indicies,match):\n",
        "  gradcam(img, model, last_conv_layer, np.argmax(label), show=True, name=idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_T6dT66fDHn"
      },
      "source": [
        "last_conv_layer = 'block4_conv3'\n",
        "for idx, (img,label) in zip(indicies,match):\n",
        "  gradcam(img, model, last_conv_layer, np.argmax(label), show=True, name=idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfAiX6_fgAGC"
      },
      "source": [
        "last_conv_layer = 'block5_conv3'\n",
        "for idx, (img,label) in zip(indicies,match):\n",
        "  gradcam(img, model, last_conv_layer, np.argmax(label), show=True, name=idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfkO4SC57Q-f"
      },
      "source": [
        "eval_list = []\n",
        "for idx in indicies:\n",
        "  eval_list.append((idx, y_test[idx],1 if y_pred_1[idx].numpy()[0] else 0))\n",
        "np.savetxt(save_path+'12x12match.txt', eval_list)\n",
        "eval_list"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}